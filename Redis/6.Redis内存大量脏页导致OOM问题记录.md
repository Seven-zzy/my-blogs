#### 问题现象：Redis连接时出现OOM，内存使用超过设置的maxmemory(1G)
#### 定位过程：
##### 1.查看redis内存使用情况：info命令，发现用户数据仅占用6.7M，内存碎片率为5.26。主从实例内存信息基本一致。
```java
# Clients
connected_clients:5
client_longest_output_list:0
client_biggest_input_buf:0
blocked_clients:0

# Memory
used_memory:6717912            6.7M
used_memory_human:6.41M
used_memory_rss:35307520       35M
used_memory_peak:440795768     440M
used_memory_peak_human:420.38M  
used_memory_lua:36864
mem_fragmentation_ratio:5.26
mem_allocator:jemalloc-3.6.0
```
##### 2.通过pmap指令查看，有大量anon内存存在。

至此初步判断为redis脏页占用了大量内存引起。（该redis db只作为cache使用，没有持久化操作，导致linux无法flush脏页至磁盘）

##### 3.查找什么操作导致了redis大量脏页产生。
###### 从info中看，实际redis中只有不到50个key，但内存使用峰值很高，怀疑此redis有频繁写入删除操作。Nosql由于大量使用内存且为了效率，一般都会使用惰性内存回收
策略，导致内存膨胀率较高。查阅了Redis官方文档，对于业务释放不使用的内存，redis会继续持有，并不归还给操作系统，以应对可能出现该内存块数据再次使用的情况。

###### 针对redis频繁写入删除操作的怀疑，刚好在前一天走读代码中发现redis更新存在先remove后add的问题。
为了对Hash进行分页，同时使用List保存了ids。为防止ids重复，对List做了先remove后add的更新操作。同时把hash的也一起这样封装了，导致了该问题。
查看业务日志，发现由于一个kafka消费的bug导致确实存在大量的redis更新操作。至此，问题已经基本定位出原因。又重新找了套环境，按照分析复现出了内存碎片率高，
redis内存上涨较快的问题。

##### 4.修改。将List改为Sorted Set，对hash和Sorted Set的更新操作，不需要remove，直接set即可。

##### 5.验证。修改后构造问题出现场景，redis内存不再持续上涨。
